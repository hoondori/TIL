# Microsoft Malware Classification Challenge 2015

문제 정의
* Malware Classification (9-class)
* **NOT Malware Detection** (two-class, anomaly detection)

데이터
* 10000/10000 Train/Test Set
* PE File payload without header, Disassembled text


## 1등 분석 (NO to overfitting)

### 관련 발표 자료
[first place source](https://github.com/xiaozhouwang/kaggle_Microsoft_Malware)
[first place story](http://blog.kaggle.com/2015/05/26/microsoft-malware-winners-interview-1st-place-no-to-overfitting/)
[first place story video](https://www.youtube.com/watch?v=VLQTRlLGz5Y&feature=youtu.be)

### 연관된 ML 기법

Feature Engineering(Generation/Selection)
* [Apriori Algorithm](https://youtu.be/Hk1zFOMLTrw) for mining frequent itemsets
* [Malware Image](http://vizsec.org/files/2011/Nataraj.pdf)
* Random forest based feature selection
 * [Feature Selection](http://machinelearningmastery.com/an-introduction-to-feature-selection/)
 * [Decision Tree](http://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf)
 * [Random Forest, Ensenble](https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/)
 * [Feature selection by random forest classifier](http://s3-us-west-2.amazonaws.com/ai2-s2-pdfs/7a80/e290230791a98e5766c2e2d078647192105f.pdf)

Model Engineering
* [Cross-validation](http://www.salford-systems.com/videos/tutorials/how-to/an-introduction-to-cross-validation)
* [Semi-supervised learning](https://mitpress.mit.edu/sites/default/files/titles/content/9780262033589_sch_0001.pdf)
* [Overfitting](https://www.youtube.com/watch?v=EQWr3GGCdzw)
* multiclass logistic regression by softmax classifier
 * [logistic regression](https://www.youtube.com/watch?v=zAULhNrnuL4)
 * [multiclass logistic regression](https://youtu.be/uGdpGFQu33g)
 * [multinominal logistic regression(softmax regression)](http://blog.datumbox.com/machine-learning-tutorial-the-multinomial-logistic-regression-softmax-regression/)
* Gradient Boost by [XgBoost](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
 * Open-sourced, most popular methods in kaggle competition

Evaluation/Visualization
* [Confusion Matrix](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)
* [Loss functions for classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification)
* [t-SNE](https://youtu.be/RJVL80Gg3lA) for high-dimensional data visualization


### 전개 과정

#### 결국은 feature engineering에서 판가름이 난다.
* simple instruction, byte count와 같은 매우 초보적인 feature들만 동원해서 log loss가 0.2에 도달
* 게임 끝? No. training set에 조금만 outlier가 있어도 곧바로 overfitting  문제가 발생
* 결론 : feature가 overfitting에 강하지 않다. feature engineering이 필요

#### Feature brainstorming
* 남의 것
 * single-byte frequency, byte 4gram, instruction count, function names and [Derived assembly features](https://www.utdallas.edu/~bxt043000/Publications/Journal-Papers/DAS/J51_A_Scalable_Multilevel_Feature_Extraction_Technique_to_Detect_Malicious_Executables.pdf)
* 제안하는 것
 * *interesting* opcode count, segment count
 * asm file pixel intensity

![pic1](https://github.com/hoondori/TIL/blob/master/images/ms_contest_1.png)


##### N-gram opcode count
* 165개의 주요 1-gram opcode : 적어도 한 asm file내에 200번 이상 출현
* Apriori  알고리즘을 이용한 2-Gram, 3-Gram, 4-Gram
* 총 7만여개의 N-gram 확보

##### Segment count
* asm file에서 각 라인별 소속 segment가 있음. ex) .text, .code, .resource
* 총 450여개의 segment 종류별 count (very sparse)

##### asm file pixel intensity image
* file의 모든 byte를 각 pixel로 대응하여 gray-scale image 만들기
* feature 개수가 매우 많음

![pic2](https://github.com/hoondori/TIL/blob/master/images/ms_contest_2.png)

#### Feature selection

벌써 feature 개수가 수만, 수십만?
* Computing도 오래 걸리고
* Overfitting이 더 심화된다

##### Random forest based feature selection
* 각 feature별로 중요도 측정, TopN을 고른다.
* Opcode N-Gram : 7만 => 4400
* Segment count : 450 => 19

##### asm file pixel intensity image의 feature selection
* 이미지 첫줄의 800 pixel만...
* 왜? 아몰랑, 그냥 이거 쓰니까 잘되...

##### 조합 결과
![pic3](https://github.com/hoondori/TIL/blob/master/images/ms_contest_3.png)

#### Model construction
* 모델 :  multiclass classification with the softmax objective
* 학습 라이브러리 : XgBoost
* Semi-supervised learning을 통한 자가 학습 데이터 증식 효과
 * 난 나를 믿어~


## 나의 결론 및 교훈
* 학습 방법론 보다는 feature brainstorming 에 focus를 맞추어서 실행 계획을 세울 것



























































